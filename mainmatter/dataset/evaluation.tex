\section{Metamodel Evaluation}
\label{sec:dataset:architecture_evaluation}

We analysed seven popular datasets recently used in literature seen as benchmarks for text extraction or object recognition. Our criteria for selection was generally open, but we ensured that usage of these datasets were from papers within at least the last 5 years. We assessed their annotation format to determine the heuristic overlap between the dataset's annotation model and our metamodel. An overview of the datasets, annotation formats and capturing software is shown in \cref{tab:dataset:metamodel_evaluation:datasets}. We summarise our analysis of directly mapping components from these formats within the \gls{adf} in \cref{tab:dataset:metamodel_evaluation:mappings}.

\vspace*{\fill}
\begin{table}[h]
  \caption[Datasets and respective annotation formats]{Differing datasets and their respective annotation formats which we have assessed for comparison with our metamodel. See \cref{ch:dataset_schemas} for serialisable annotation formats.}
  \label{tab:dataset:metamodel_evaluation:datasets}
  \tablefit{
    \begin{tabular}{@{}ll|p{0.21\textwidth}p{0.24\textwidth}lp{0.25\textwidth}@{}}
      \toprule
        \textbf{Dataset} &
        \textbf{Reference} &
        \textbf{Challenge} &
        \textbf{Format} &
        \textbf{Listing} &
        \textbf{Annotation Tooling} 
      \\
      \midrule      
        MS \glsac{coco} &
        \citep{Lin:2014vma, Chen:2015ur} &
        Object Recognition &
        MS \glsac{coco} \glsac{json} &
        \ref{lst:dataset_schemas:ms_coco} &
        Custom Tool via \glsac{amt} 
      \\
        \glsac{coco}-Text & 
        \citep{Veit:2016vj} & 
        Text Reading & 
        MS \glsac{coco} \glsac{json} & 
        \ref{lst:dataset_schemas:coco_text} & 
        Tool from \citet{Matera:2014wq} via \glsac{amt}
      \\
        \glsac{icdar} 03--11 &
        \citep{Lucas:2003iw, Lucas:2005bq, Chen:2011ul} &
        Text Reading &
        \glsac{xml} &
        \ref{lst:dataset_schemas:icdar_03-11} &
        Custom Tool via Java Applet
      \\
        \glsac{icdar} 11--15 &
        \citep{Shahab:2011hq,Karatzas:2013by,Karatzas:2015tj} &
        Text Reading &
        \glsac{xml} &
        \ref{lst:dataset_schemas:icdar_11-15} &
        \glsac{cvc} \glsac{apep} \cite{Karatzas:2014bt}
      \\
        ImageNet &
        \citep{JiaDeng:2009dl} &
        Object Recognition &
        \glsac{pascal} \glsac{voc} &
        \ref{lst:dataset_schemas:pasvoc} &
        Custom Tool via \glsac{amt}
      \\
        \glsac{sun} &
        \citep{Xiao:2010td} &
        Object Recognition &
        \glsac{pascal} \glsac{voc} &
        \ref{lst:dataset_schemas:pasvoc} &
        LabelMe \cite{Russell:2008wm}
      \\
        \glsac{pascal}-Context &
        \citep{Mottaghi:2014ie} &
        Object Recognition &
        \glsac{pascal} \glsac{voc} (2012) &
        \ref{lst:dataset_schemas:pasvoc_2012} &
        Custom Tool similar to LabelMe \cite{Russell:2008wm}
      \\
        Synth90K &
        \citep{Jaderberg:2016wj, Jaderberg:2014uy, Gupta:2016ws} &
        Text Reading &
        MATLAB Binary &
        N/A &
        SynthText\footnotemark
      \\
        \glsac{svhn} &
        \citep{Netzer:2011to} &
        Text Reading &
        MATLAB Binary &
        N/A &
        Custom Tool via \glsac{amt}
      \\
      \bottomrule
    \end{tabular}
  }
\end{table}

\footnotetext{\url{https://github.com/ankush-me/SynthText} last accessed 15 August 2017.}

\def \mmmapyes {$\bullet$}
\def \mmmapno  {\ }

\begin{table}[h]
  \caption[Components present in the ADF compared to other annotation formats]{Presence of components of popular annotation formats within the \gls{adf} metamodel. See \cref{ch:dataset_mappings} for detailed mapping.}
  \label{tab:dataset:metamodel_evaluation:mappings}
  \tablefit{
    \begin{tabular}{@{}ll|cccc@{}}
      \toprule
        \bfseries \gls{adf} &
        &
        \bfseries \glsac{coco} \glsac{json} &
        \bfseries \glsac{pascal} \glsac{voc} \gls{xml} &
        \bfseries \gls{icdar} \gls{xml} &
        \bfseries MATLAB Binaries
      \\
      \midrule
        \bfseries Feature &
        Image-Level &
        \mmmapyes{} &
        \mmmapno{} &
        \mmmapno{} &
        \mmmapno{}
      \\
        &
        Segment-Level &
        \mmmapyes{} &
        \mmmapyes{} &
        \mmmapyes{} &
        \mmmapyes{}
      \\
        \bfseries Annotation &
        Label &
        \mmmapyes{} &
        \mmmapyes{} &
        \mmmapyes{} &
        \mmmapyes{}
      \\
        &
        Category &
        \mmmapyes{} &
        \mmmapyes{} &
        \mmmapyes{} &
        \mmmapno{}
      \\
        &
        Collection &
        \mmmapno{} &
        \mmmapyes{} &
        \mmmapyes{} &
        \mmmapyes{}
      \\
        &
        Boundary &
        \mmmapyes{} &
        \mmmapyes{} &
        \mmmapyes{} &
        \mmmapyes{}
      \\
        \bfseries Attribute &
        Derived &
        \mmmapyes{} &
        \mmmapyes{} &
        \mmmapyes{} &
        \mmmapyes{}
      \\
        &
        Explicit  &
        \mmmapno{} &
        \mmmapno{} &
        \mmmapno{} &
        \mmmapyes{}
      \\
      \bottomrule
    \end{tabular}
  }
\end{table}
\vspace*{\fill}
\clearpage

For each of these datasets, the annotation capturing strategies differed and, similarly, the annotation was either encoded as a binary MATLAB\footnote{MATLAB is a registered trademark of The MathWorks, Inc.} file or serialised in plain text (either as \gls{xml} or \glsac{json}). In the case where annotations were serialised in \gls{xml}, a popular format was the \glsac{pascal}\footnote{\gls{pascal}}-\gls{voc} \citep{Everingham:2009dq} format. This format was made popular with the \glsac{pascal} \gls{voc} Challenges \citep{Everingham:2015tv,Everingham:2010wz,Everingham:2005vr,Everingham:2009dq}, though custom annotation formats in \gls{xml} are also present (e.g., \gls{icdar}).

\subsection{MS COCO and COCO-Text JSON Format}

The Microsoft \gls{coco} \citep{Lin:2014vma} dataset and its image captions extension \citep{Chen:2015ur} require all annotations to be labelled to one of 91 specified categories. This is done using a three-step annotation pipeline consisting of: (1) labelling all categories of objects in the image, (2) spotting all instances of these characters, and (3) segmenting the instances using a polygon. For our analysis, we focus only on the Object Keypoint and Image Caption Annotation challenges.

We mapped the annotation concepts from both MS \gls{coco} (and its text-reading equivalent \gls{coco}-Text) and mapped components to the \gls{adf}. As shown in \cref{fig:dataset:metamodel_evaluation:mapping:coco}, these high-level components are complete when expressed in \gls{adf}, and the mapping is powerful enough to compute required components of the \gls{coco}-based annotation formats to derived calculations within \gls{adf}.

Our metamodel did not consider using \gls{rle} to encode multiple boundaries (polygons), as is used when \textit{Is Crowd} is set to 1 to indicate multiple instances in the image. However, we still capture this concept under the a form of a \textit{Boundary}. Additionally, we found that the \textit{Is Crowd} is the only mapping of all the formats that relates to an \gls{adf} \textit{Image-Level Feature}.

\subsection{PASCAL VOC XML Format}

The \gls{pascal} \gls{voc} \gls{xml} format showed the most consistency with \gls{adf}'s variant \textit{Annotation} sub-types (\cref{fig:metamodel_class_diagrams:feature}), along with the \gls{icdar} \gls{xml} format. The \gls{pascal} \gls{voc} format shows direct mappings to our components, such as \textit{Bounding Box} to an \textit{Annotation}'s equivalent, \textit{Boundary}. Additionally, a collection of \textit{Action}s (as given in the 2012 format to extend the \textit{Pose} of a person feature) can be used as a list of variant \textit{Category} annotations. Further mappings can be seen in \cref{fig:dataset:metamodel_evaluation:mapping:pascal}.

\subsection{ICDAR XML Formats}

The two \gls{icdar} datasets (including the 2011 update) can also be captured within an \gls{adf}. We present this mapping in \cref{fig:dataset:metamodel_evaluation:mapping:icdar}. Most notably, we map the \textit{Text Line}, \textit{Word}, \textit{Atom} and \textit{Text Parts} as \gls{adf} \textit{Collection}s. The \textit{Don't Care} component is a flag to notify if a line is illegible, which we can indicate using a \textit{Boolean} annotation (a \textit{Category}). The explicit \textit{Color} type in the \gls{icdar} 2011--2015 format can also be mapped to a \textit{Label} (more specifically, the \textit{Colour} label).

\subsection{MATLAB-Encoded Binaries}

To investigate the mapping of variant MATLAB binaries, we inspected the relevant guides on how to read the cell-arrays for both the SynthText 90k\footnoteurl{http://www.robots.ox.ac.uk/~vgg/data/scenetext/readme.txt}{17 August 2017} and \gls{svhn}\footnoteurl{http://ufldl.stanford.edu/housenumbers/\#downloads}{17 August 2017} datasets. Of the formats investigated, these were the simplest, and were the only ones to directly map an \textit{Explicit Attribute} (for Synth90k). These are shown in further detail within \cref{fig:dataset:metamodel_evaluation:mapping:matlab}.

% Evaluate the meta model using a herustic overlap to check if our metamodel is complete. Do this against three existing and popular datasets: COCO text, ICDAR?, SVHN?. Check Scott's thesis on his eval strats to see if the metamodel is complete, consistsent and expressive enough against other parts of the meta model, and where we could potentially fill gaps.

% COCO Text \cite{}, MS Coco  \cite{} (JSON)

% ICDAR03-11 XML (\cite{Lucas:2005hl})

% ImageNet \cite{JiaDeng:2009dl}, stored in the PASCAL-VOC Format

% Synth90k \cite{Gupta:2016ws}
% http://www.robots.ox.ac.uk/~vgg/data/scenetext/SynthText.zip
% Bounding Boxes (x y)
% SynthText.zip (size = 42074172 bytes (41GB)) contains 858,750 synthetic
%scene-image files (.jpg) split into 200 directories, with 
%7,266,866 word-instances, and 28,971,487 characters.
%
%Ground-truth annotations are contained in the file "gt.mat" (Matlab format).
%The file "gt.mat" contains the following cell-arrays, each of size 1x858750:
%
%  1. imnames :  names of the image files
%
%  2. wordBB  :  word-level bounding-boxes for each image, represented by
%                tensors of size 2x4xNWORDS_i, where:
%                   - the first dimension is 2 for x and y respectively,
%                   - the second dimension corresponds to the 4 points
%                     (clockwise, starting from top-left), and
%                   -  the third dimension of size NWORDS_i, corresponds to
%                      the number of words in the i_th image.
%
%  3. charBB  : character-level bounding-boxes,
%               each represented by a tensor of size 2x4xNCHARS_i
%               (format is same as wordBB's above)
%
%  4. txt     : text-strings contained in each image (char array).
%               
%               Words which belong to the same "instance", i.e.,
%               those rendered in the same region with the same font, color,
%               distortion etc., are grouped together; the instance
%               boundaries are demarcated by the line-feed character (ASCII: 10)
%
%               A "word" is any contiguous substring of non-whitespace
%               characters.
%
%               A "character" is defined as any non-whitespace character.

% SVHN \cite{Netzer:2011to} - Each element in digitStruct has the following fields: name which is a string containing the filename of the corresponding image. bbox which is a W array that contains the position, size and label of each digit bounding box in the image. Eg: digitStruct(300).bbox(2).height gives height of the 2nd digit bounding box in the 300th image. 

% SUN \cite{Xiao:2010td}; http://people.csail.mit.edu/torralba/publications/memories.pdf <- annotation
% Scene recognition. Does not specify their annotation language for aspects within a scene.  
% LabelMe annotation tools.

%%%% 

% Evaluate argus UI with others, such as the annotation of COCO in the COCO dataset paper (Fig 12).

% SUN - http://labelme.csail.mit.edu/Release3.0/browserTools/php/matlab_toolbox.php; http://people.csail.mit.edu/brussell/research/AIM-2005-025-new.pdf

% - https://docs.scaleapi.com/#polygon-annotation
%% Large annotation 
%% Annotation as a service

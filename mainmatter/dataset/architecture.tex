\section{A Data-Capturing Metamodel}
\label{sec:dataset:architecture}

Why should we care about the architecture of our dataset capturing, so long as the \gls{ai} is well-trained? This question fundamentally leads to the \textit{provenance} or \textit{lineage} of our data: not all training data is initially perfect. At times, we will need to modify our training data, or in the cases of some \gls{ai} systems, training data is re-fed back into the system from the \gls{ai} itself. 

Hence, it is imperative to understand how the transition and flow of data occurs \citep{Cui:2003im,Ikeda:2009ca,Buneman:2000bn} in these \gls{ai} systems. Where does it comes from? How is it derived and updated, and how might this affect our trained \gls{ai} models over time? Without a thorough understanding of data provenance, tracing potential defects within training models becomes difficult. It may be a difference in the data training format, or a mismatch of values on the same training data. Systematic record-keeping is critical to overcome this challenge.

We propose a grammar to capture the vocabulary of our metamodel and  approach of a data-capturing architecture. As such a grammar is textual, it can be version controlled: therefore data provenance is recorded, which allows us to source how the mutations in training data can occur and and what effect this has our \gls{ai} systems. From an exploratory analysis of developing a concrete implementation of this system (\cref{sec:dataset:architecture:methodology}), we propose a conceptual metamodel that can be used to annotate and formally describe image data for training purposes (e.g., frames of a video, images of natural scenes). This metamodel is largely inspired by the works of \citet{Wickham:2010hy, Wickham:2007tu} and \citet{Moody:2009vo}.

\input{mainmatter/dataset/architecture/methodology.tex}
\input{mainmatter/dataset/architecture/what_to_capture.tex}
\input{mainmatter/dataset/architecture/metamodel.tex}

\section{Evaluation Strategies}
\label{sec:evaluation:strategies}

To apply a robust evaluation on our pipeline, we randomly sampled 200 images from our dataset that were not used in training or validation. Half of these evaluation images only contained one bib marked up in the image (100 \glspl{rbn}), while the other half had more than one bib in the image (a total of 221 \glspl{rbn}).

We used a two-stage pass on this set: the first pass applies to the first hundred images that we consider as an `ideal' case for the pipeline to handle (i.e., to detect the only bib in the image). The second pass considers the `realistic' case where there is more than one bib per image. Henceforth, we refer to our two subsets as `ideal' and `realistic'.

In addition to categorising the sets as ideal and realistic, we also considered what the minimal amount of training data required is to make the bib detection robust. We evaluated three additional trained models using \frcnn{} for bib detection on 1, 100 and 500 training bibs, following the process outlined in \cref{sec:dataset:postprocessing:augmentation}. Each of these images are extracted from the same source dataset consisting of the 722 training samples\footnote{We refer to this as `all' images in our training dataset.} indicated in \cref{tab:dataset:postprocessing:augmentation_quantities}. 

Lastly, we evaluated how the person filtering stage (\cref{sec:processing_pipeline:person_filtering}) affects the overall performance of the pipeline. The permutations of these evaluation methods results in a total of sixteen different evaluations, shown in \cref{tab:evaluation:overview}. We generated a dash-separated identifier to refer to each of these evaluations based on \textit{I} or \textit{R} for \textit{ideal} and \textit{realistic}, respectively, then the number of training images (using \textit{all} to refer to all training images) and lastly whether to crop on the image or not crop (\textit{CR} for \textbf{cr}op, and \textit{NC} for \textbf{n}o \textbf{c}ropping).

\csvtable{csv/summary_stats.csv}
         {Summary of Evaluations}
         {tab:evaluation:overview}
         {llll}
         {1=\EvaluationID, 3=\EvaluationSet, 4=\TrainingImgs, 5=\CropHuman}
         {\textbf{Evaluation ID} & \textbf{Evaluation Set} & \textbf{Training Bibs} & \textbf{Crop Human}}
         {\textbf{\EvaluationID} & \EvaluationSet & \TrainingImgs & \CropHuman}

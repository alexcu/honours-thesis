\chapter{Dataset}
\label{ch:dataset}

How do we capture large and annotated datasets to train an \gls{ai} model at scale? What is an annotation, and how can we constrain them? These are important questions when supplying data to train graphics-based \glspl{ai}. In this chapter, we discuss a theoretical data tagging metamodel that enables us to concisely describe the architecture of how quality labelled data is captured for these purposes. Additionally, we discuss a partial implementation of this metamodel which we have used to gather our training dataset.

\section{A Data-Capturing Architecture}
\label{sec:dataset:architecture}

Why should we care about the architecture of our dataset capturing, so long as the \gls{ai} is well-trained? This question fundamentally leads to the \textit{provenance} or \textit{lineage} of our data: not all training data is initially perfect. At times, we will need to modify our training data, or in the cases of some \gls{ai} systems, training data is re-fed back into the system from the \gls{ai} itself. 

Hence, it is imperative to understand how the transition and flow of data occurs \citep{Cui:2003im,Ikeda:2009ca,Buneman:2000bn} in these \gls{ai} systems. Where does it comes from? How is it derived and updated, and how might this affect our trained \gls{ai} models over time? A lack of understanding in training data provenance for large-scale \glspl{ai} implies difficulty in sourcing the cause of errors due to these mutations. It may be a difference in the data training format, or a mismatch of values on the same training data. Overcoming this without systematic record-keeping is challenging.

We therefore propose a grammar to describe layered-approach of a data-capturing architecture. As such a grammar is textual, it can be version controlled: therefore data provenance is recorded, which allows us to source how the mutations in training data can occur and and what effect this has our \gls{ai} systems. From an exploratory analysis of developing this architecture (Section~\ref{sec:dataset:architecture:methodology}), we propose a theoretical and generalisable metamodel that can be used to capture any form image data for training purposes (e.g., frames of a video, images of natural scenes). This metamodel is largely inspired by the works of \citet{Wickham:2010hy, Wickham:2007tu} and \citet{Moody:2009vo}.

\subsection{Methodology}
\label{sec:dataset:architecture:methodology}

A partial implementation of this metamodel, which we call \textit{Argus}\footnoteurl{http://www.deakin.edu.au/~ca/argus}{5 July 2017}\footnote{Inspired by the \textit{all-seeing} giant of the same name from Greek mythology: ``With his multiple sets of eyes, Argus could see nearly everything in his vicinity''. See \url{http://www.loggia.com/myth/argus.html}.}, was developed for the purposes of capturing our dataset. Refer to Section~\ref{} for more about this implementation.

% Logical implementation vs declarative?

\subsection{What to Capture?}
\label{sec:dataset:architecture:what_to_capture}

What features do we want to capture from our dataset? To answer this question, we need to consider what information we see as relevant in a standard marathon photo (such as Figure~\ref{fig:introduction:background:sample_rbns}). There are two annotations to describe features of the entire photo:

\begin{enumerate}
  \item whether or not the photo is considered as \textbf{crowded} (\textsc{true} or \textsc{false}), and
  \item an \textit{optional} collection of \textbf{runners} in the photo, given that the photo is not crowded.
\end{enumerate}

% TODO: Example of crowded photo and photo with no runners

We train a \gls{nn} based on photos that are not crowded; photos that are considered crowded are typically not desirable as runners prefer photos where they are the key subject. We therefore discard these photos to remove any bias on the network. We can only tag runners on the condition that the photo is not marked as  crowded. Additionally, in some photos, all \glspl{rbn} are missing within the photo (e.g., hidden behind other runners, cropped out of view) and some photos contain no runners at all. We therefore identify this as an \textit{optional} collection as we must associate an \gls{rbn} to a runner---the photo is not crowded, but there are no runners to tag, thereby making the annotation optional. Thus, an \textit{implicit attribute} exists with such a collection: the \textit{count} of the runners in a photo is something we can automatically count.

We identify the following two annotations of what we would label for a given runner's bib sheet, a feature important to identify \glspl{rbn}:

\begin{enumerate}
  \item a polygon around the runner's \textbf{bib sheet} that contains the $x$ and $y$ coordinates, given that there are exactly four vertices of this polygon
  \item a string with the runner's \textbf{\gls{rbn}}, once the bib sheet is tagged (exists).
\end{enumerate}

A further feature that is important is the runner's face region, which could compromise of five annotations:

\begin{enumerate}
  \item a rectangle around the runner's \textbf{face} that contains the two $x$ and $y$ coordinates of the two opposite vertices, given that the $bottom$ of this rectangle are above the $top$ of the bib sheet,
\end{enumerate}

\noindent
and once this face bounds has been tagged, more annotations can be extracted:

\begin{enumerate}
  \setcounter{enumi}{1}
  \item whether or not the runner is \textbf{wearing} a \textbf{hat} (\textsc{true} or \textsc{false}),
  \item whether or not the runner is \textbf{wearing} \textbf{(sun)glasses} (\textsc{true} or \textsc{false}), and
  \item the \textbf{gender} of the runner (\textsc{male}, \textsc{female}, or \textsc{unsure}).
\end{enumerate}

The bib sheet polygon would contain four \textit{explicit attributes} required as input by the tagger: the coordinates of its four vertices, $\{ (x_{1}, y_{1}), (x_{2}, y_{2}), (x_{3}, y_{3}), (x_{4}, y_{4}) \}$. Similarly, there are two explicit attributes required for the runner's face rectangle: the coordinates of the top left and the bottom right of the rectangle (i.e., the two opposite vertices) of the runner's face bounds, $\{ (x_{1}, y_{1}), (x_{2}, y_{2}) \}$. As both of these are shapes, we can automatically calculate the implicit attributes from these coordinates:  $\{ top, left, bottom, right, width, height \}$.

% TODO: Example of bib + face coordinates and RBN

%Given that the face region is tagged, we can then annotate:
%
%\begin{enumerate}
%  \item the `base classifications' of the person, and
%  \item the `colour classifications' of the person.
%\end{enumerate}

We can now identify another feature, the prominence of the runner within the photo. These would consist of three annotations:

\begin{enumerate}
  \item whether or not the runner's \textbf{face} is \textbf{visible} (\textsc{true} or \textsc{false}),
  \item whether or not the runner is \textbf{blurry} (\textsc{true} or \textsc{false}),
  \item the \textbf{\gls{lop}} that the runner buys the photo (\textsc{no}, \textsc{maybe}, \textsc{yes}),
\end{enumerate}

A runner's face is considered `invisible' if it is obstructed by another object, cropped out of the image, or if the runner is looking down. Runners are more likely purchase a photo if they are looking at the camera, and likewise if they are blurry in the image; these therefore have a significant impact on their prominence. We also define the \glsx{lop} value as a qualitative metric. We ask the data tagger to `picture' themselves as runner, and if so, we ask if they would purchase it. We then use this metric to train positive samples of good photos (\gls{lop} = \textsc{yes}) and samples of bad photos (\gls{lop} = \textsc{no}). Where \textsc{maybe} is provided, the runner is ignored to prevent training the network with indeterminate samples.

% TODO: Example of face visibility (cropped, hidden and looking down)
% TODO: Example of LoP

Another feature we can capture is a way to identify runners by the colour of their clothing. (Within our dataset, various clothing items of distinct colours are given to runners for particular races.) These features comprise of four annotations:

\begin{enumerate}
  \item an \textit{optional} \textbf{colour} of the runner's \textbf{hat}, given that they were annotated as wearing a hat,
  \item the \textbf{colour} of the runner's \textbf{shirt},
  \item an \textit{optional} \textbf{colour} of the runner's \textbf{shorts}, and
  \item an \textit{optional} \textbf{colour} of the runner's \textbf{shoes}.
\end{enumerate}

% TODO: Example of colour matching

The colour of a runner's shirt is required, as we expect that a bib would be detected on the shirt of a runner (which would be tagged). The other colour annotations are all optional, as it is likely that some of these clothing items may be cropped out of the photo or is not visible. Furthermore, the hat colour can only be specified on the condition that the tagger has marked the runner has wearing a hat.

We can segment groups of the annotations we extract into features of two categories: annotations that feature at the \textit{image}-level and those that do not, which we call \textit{segment}-level features. The image-level features are those which apply to the entire image (i.e., $PhotoCrowded$, $Runners$). Those that apply at the segment level can be grouped into what types of features we are extracting: the runner's bib, face, their prominence and their colour identification.

In summary, we have identified a total of 5 features of 15 annotations, which are summarised within Table~\ref{tab:dataset:annotation_summary}. Each of these annotations can be classified with a name and type, conditions for the annotation to be valid, dependencies on other annotations to exist before the annotation can be made, explicit attributes which the tagger must specify, implicit attributes which we can automatically compute, possible values which limit the range of data for that annotation, and whether or not the annotation is optional.

\begin{landscape}

\begin{table}[p]
  \centering
  \caption[Summary of annotations captured in the dataset]{\centering A summary of the annotations we wish to capture from our dataset. Image and segment-level features are separated using the double line.}
  \label{tab:dataset:annotation_summary}
  \tablefitlandscape{
    \begin{tabular}{lllllllll}
      \toprule
        \textbf{Feature} &
        \textbf{Name} &
        \textbf{Type} &
        \textbf{Conditions} &
        \textbf{Dependencies} &
        \textbf{Explicit Attributes} &
        \textbf{Implicit Attributes} &
        \textbf{Possible Values} &
        \textbf{Optional}
      \\
      \midrule
        \multirow{2}{*}{Image-Level} &
        $PhotoCrowded$ &
        Boolean &
        None &
        None &
        None &
        None &
        $\{ \textsc{true}, \textsc{false} \}$&
        No
      \\
        &
        $Runners$ &
        Collection &
        $\{ PhotoCrowded = \textsc{false} \}$ &
        None &
        None &
        $\{count\}$ &
        N/A &
        Yes
      \\
      \midrule
      \midrule
        \multirow{2}{*}{Bib} &
        $BibSheet$ &
        Polygon &
        $\{ vertices = 4 \}$ &
        None &
        $\{ x_{1} \dots x_{4}, y_{1} \dots y_{4} \}$ &
        $\{ top, left, bottom, right, width, height \}$ &
        N/A &
        No
      \\
        &
        $\gls{rbn}$ &
        String &
        None &
        $BibSheet$ &
        None &
        None &
        N/A &
        No
      \\
      \midrule
        \multirow{2}{*}{Face} &
        $FaceBounds$ &
        Rectangle &
        $\{ bottom > BibSheet_{top} \}$  &
        $BibSheet$ &
        $\{ x_{1}, y_{1}, x_{2}, y_{2}\}$ &
        $\{ top, left, bottom, right, width, height \}$ &
        N/A &
        No
      \\
        &
        $FaceVisible$ &
        Boolean &
        None &
        $FaceBounds$ &
        None &
        None &
        $\{ \textsc{true}, \textsc{false} \}$&
        No
      \\
        &
        $WearingHat$ &
        Boolean &
        None &
        $FaceBounds$ &
        None &
        None &
        $\{ \textsc{true}, \textsc{false} \}$&
        No
      \\
        &
        $WearingGlasses$ &
        Boolean &
        None &
        $FaceBounds$ &
        None &
        None &
        $\{ \textsc{true}, \textsc{false} \}$&
        No
      \\
        &
        $Gender$ &
        Category &
        None &
        $FaceBounds$ &
        None &
        None &
        $\{ \textsc{male}, \textsc{female} \} $&
        No
      \\
      \midrule
        \multirow{2}{*}{Prominence} &
        $\gls{lop}$ &
        Category &
        None &
        $FaceBounds$ &
        None &
        None &
        $\{ \textsc{no}, \textsc{maybe}, \textsc{yes} \}$ &
        No
      \\
        &
        $Blurry$ &
        Boolean &
        None &
        $FaceBounds$ &
        None &
        None &
        $\{ \textsc{true}, \textsc{false} \}$&
        No
      \\
      \midrule
        \multirow{2}{*}{Colours} &
        $ShirtColor$ &
        Color &
        None &
        $FaceBounds$ &
        $\{ red, green, blue \}$ &
        None &
        N/A &
        No
      \\
        &
        $ShoeColor$ &
        Color &
        None &
        $FaceBounds$ &
        $\{ red, green, blue \}$ &
        None &
        N/A &
        Yes
      \\
        &
        $ShortsColor$ &
        Color &
        None &
        $FaceBounds$ &
        $\{ red, green, blue \}$ &
        None &
        N/A &
        Yes
      \\
        &
        $HatColor$ &
        Color &
        $\{ WearingHat = \textsc{true} \}$ &
        $FaceBounds$ &
        $\{ red, green, blue \}$ &
        None &
        N/A &
        Yes
      \\
      \bottomrule
    \end{tabular}
  }
\end{table}

\end{landscape}

% Run through an example of what it is we need to capture for our study & why (Refer to meeting 1 notes).
% Activity diagram of things we need to classify (break down diagram).

\subsection{Describing the Metamodel}

% Conceptualise what we have in our workflow as a meta model.
% What are the components, and constraints? Discuss why we needed to add in constraints... Feedback from India

In the example above, we have seen some components that make up an annotation. We can now develop our metamodel using the terminology described: types, conditions and dependencies, explicit and implicit attributes, possible values, and
    optionality.

\paragraph{Types} We have so far identified \textit{Boolean}, which is just a restricted \textit{Category} consisting of the possible values $\{ \textsc{true}, \textsc{false} \}$. We have \textit{Polygon}s and \textit{Rectangle}s, and therefore we must conceptualise some form of \textit{Shape}. We also have \textit{Colour}s, which can be represented as an RGB represented hexadecimal \textit{String}. This only leaves our \textit{Collection} of runners: a data type which encompasses multiple annotations. We therefore a data schema that is  generalisable from \textit{Strings}, \textit{Shapes}, \textit{Collections} and \textit{Categories}.

\paragraph{Conditions} A condition is simply a construction rule which disallows a data tagger to create an annotation until a condition is met by the annotator. For instance, we can never have a $BibSheet$ above a $FaceBounds$, so when we can specify this as a condition. These are not bound to just geometric constraints; we also see that we cannot tag $Runners$ if the $PhotoCrowded$ annotation is marked as \textsc{true}.

\paragraph{Dependencies} Some annotations are dependent on others existing. These are construction rules which we label as dependencies. We cannot annotate an \gls{rbn} if there is no $BibSheet$ annotated. Likewise, a $FaceBounds$ is dependent on where the $BibSheet$ is, and so the $BibSheet$ must be marked up first. These dependencies add order in which features are being extracted, which is important to the tagger marking up the photo.

\paragraph{Possible Values} In the instance where a data type must be restricted to a set of values (i.e., \textit{Categorical} data), we set what those restrictions are as a construction rules of our annotations.

\paragraph{Optionality} Not all annotations are needed---in these cases the annotation is considered optional, but by default is required.

\paragraph{Attributes} Attributes exist to as a means capture further information about an annotation. We represent attributes in our metamodel as either implicit or explicit. As stated in Section~\ref{sec:dataset:architecture:what_to_capture}, those attributes which are required for the tagger to manually markup are explicit (i.e., manually must be added) whereas those that can be computed by the system are implicit (i.e., inferred from explicit attributes).

These begin to form layers of data within the images we annotate. This layer can then be represented in a hierarchical data format (e.g., a tree) to describe the layering of a fully annotated photo. % TODO: Insert fully annotated photo



 We therefore highlight some components which allow us to describe this architecture in a more generalisable way.


Runners: [Collection]

\begin{enumerate}
  \item Annotation, and further, an optional annotation
  \item A condition
\end{enumerate}

\subsection{The Data Capturing Process}

Now that we have defined which annotations exist, we describe the process by which we are able to capture this data. At its core, this is an \gls{etl} process: we extract the data from our 

\subsection{Layering}



% motivation
% methodology
% a layered data tagging metamodel
%% definitions (data structures)
%% layering
%% workflows
% implementation
%% findings (productivity)
%% quality

\chapter{Dataset}
\label{ch:dataset}

How do we efficiently create, organise and use labelled datasets to permit useful machine learning? What is a label, and how can we constrain them? In this chapter, we present a offers the metamodel that offers the conceptual vocabulary and an organising principle to improve efficiency of labelling.

\input{mainmatter/dataset/architecture.tex}
\input{mainmatter/dataset/process.tex}
\input{mainmatter/dataset/postprocessing.tex}
\input{mainmatter/dataset/evaluation.tex}

\newpage
\section{Conclusions}

This chapter describes an inductive approach used for developing a generalisable metamodel from empirical iterations of systems development to gap the missing epistemology in the area of data capturing architectures. By developing a concrete implementation and reflecting upon the implementation, we were able to refine and discover a concrete metamodel. This metamodel alleviates the difficulties in maintaining data provenance when training \gls{ai} models by making the serialisable, the therefore trackable with version control systems.

How may this metamodel be used elsewhere? It seems easily adaptable to similar areas in the field of computer vision processing, such as \gls{lpr} or \gls{tsr}. However, could we move this to beyond static images? Future works may extend our metamodel to multiple image frames, and therefore annotating videos frame-by-frame is within reach. Some current limitations of the metamodel (in its current form) is the difficulty to apply it to non-vision-related topics, such as \gls{nlp}, audio processing or sensor data processing. Some key concepts may be missing from our metamodel in these areas, and we expect that these can be closed by researchers with relevant domain expertise who can further extend it. We also propose that our approach could be used as a background for future works in metamodel development.

Additionally, we have shown that our metamodel is consistent with four popular existing data formats, and therefore is aligned to requirements of previous annotation systems. Hence, adapters can be used to map such formats, thereby unifying all datasets into one readable format which can be used for \gls{ai} training and validation. We leave this open for future work.

\bigskip

\noindent
The primary contributions of this chapter are:

\begin{itemize}
  \item an approach to define prominence of marathon runners that is quantifiable,
  \item a metamodel that describes a schema to annotate large image datasets, and
  \item an exploratory methodological approach in designing a metamodel from a concrete system.
\end{itemize}

\noindent
Minor contributions in this chapter include:

\begin{itemize}
  \item a basic comparison of our metamodel against existing image-based datasets, and
  \item an overall pipeline in how to handle and deal with data provenance in \gls{ai} models.
\end{itemize}

% Concluding remarks:
% - Speculate if this metamodel is extensible?
% -- Videos?
% -- Audio?
% -- LIDAR + RFID / Sensor Data?
% - Is the methodological approach extendable?
% -- Using a concrete system to redevelop back toward a metamodel?
% - Perhaps compare with how ICDAR was annotated
% - Compare with the online websites... like integrating with Scale and the other thing?
% - "Left for future research"


% motivation
% methodology
% a layered data tagging metamodel
%% definitions (data structures)
%% layering
%% workflows
% implementation
%% findings (productivity)
%% quality
